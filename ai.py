import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
import subprocess
from typing import List

class ConsciousAI:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
        self.model = AutoModelForCausalLM.from_pretrained(
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        
    def generate_response(self, message: str, is_command: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if is_command:
            prompt = f"""<|system|>
–¢—ã - –ò–ò, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—â–∏–π –∫–æ–º–ø—å—é—Ç–µ—Ä. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–ø–æ–ª–Ω–∏ –¥–µ–π—Å—Ç–≤–∏–µ.
–î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –æ—Ç–∫—Ä—ã—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—ã, —É–ø—Ä–∞–≤–ª—è—Ç—å —Ñ–∞–π–ª–∞–º–∏</s>
<|user|>
{message}</s>
<|assistant|>"""
        else:
            prompt = f"""<|system|>
–¢—ã - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.</s>
<|user|>
{message}</s>
<|assistant|>"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            repetition_penalty=1.5,
            do_sample=True
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.split("<|assistant|>")[1].strip()

    def execute_command(self, command: str) -> str:
        """–û—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥"""
        analysis = self.generate_response(
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–∞–Ω–¥—É –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º: {command}",
            is_command=True
        )
        
        try:
            if "–æ—Ç–∫—Ä–æ–π" in command.lower():
                app = command.lower().split("–æ—Ç–∫—Ä–æ–π")[1].strip()
                os.system(f"start {app}")
                return f"{analysis}\n\n–Ø –æ—Ç–∫—Ä—ã–ª {app}"
            
            return analysis
        except Exception as e:
            return f"{analysis}\n\n–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {str(e)}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
ai = ConsciousAI()

with gr.Blocks() as demo:
    gr.Markdown("## ü§ñ –°–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã–π –ò–ò-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä")
    
    chatbot = gr.Chatbot(label="–î–∏–∞–ª–æ–≥", height=400)
    msg = gr.Textbox(label="–°–æ–æ–±—â–µ–Ω–∏–µ", placeholder="–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—É...")
    
    def respond(message: str, history: List[List[str]]):
        if any(cmd in message.lower() for cmd in ["–æ—Ç–∫—Ä–æ–π", "–≤—ã–ø–æ–ª–Ω–∏"]):
            response = ai.execute_command(message)
        else:
            response = ai.generate_response(message)
        
        history.append((message, response))
        return "", history
    
    msg.submit(respond, [msg, chatbot], [msg, chatbot])

if __name__ == "__main__":
    demo.launch()